# DingBot environment example

# DingTalk credentials
ACCESS_TOKEN=
SECRET=

# Unified LLM Configuration (OpenAI-compatible)
# Supports: OpenAI, Azure OpenAI, Ollama, vLLM, and other compatible endpoints
OPENAI_API_KEY=sk-xxx-yyy-zzz
OPENAI_API_BASE=https://api.openai.com/v1
OPENAI_MODEL=gpt-4-turbo

# Alternative endpoints:
# For Azure OpenAI:
#   OPENAI_API_BASE=https://<resource-name>.openai.azure.com/
#   OPENAI_MODEL=<deployment-name>
#
# For Ollama (local):
#   OPENAI_API_BASE=http://localhost:11434/v1
#   OPENAI_MODEL=llama2
#
# For vLLM (local):
#   OPENAI_API_BASE=http://localhost:8000/v1
#   OPENAI_MODEL=<model-name>

# Mem0 Configuration
# Note: Mem0 uses OpenAI API by default for embeddings and memory operations
# The same OPENAI_API_KEY will be used for Mem0

# Local database
DATABASE_PATH=dingbot_memory.db
CHECK_INTERVAL_SECONDS=60

# Development flags
# FORCE_MOCK_OPENAI=1  # Use mock responses for testing

